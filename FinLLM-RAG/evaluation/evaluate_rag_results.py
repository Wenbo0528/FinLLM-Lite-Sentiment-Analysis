import json
import os
from pathlib import Path
import logging
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class RAGEvaluator:
    def __init__(self, results_path):
        self.results_path = results_path
        self.results = self._load_results()
        # ä¿®æ”¹è¾“å‡ºç›®å½•ä¸º results
        self.output_dir = Path(__file__).parent.parent / "results"
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def _load_results(self):
        """åŠ è½½æ¨ç†ç»“æœ"""
        with open(self.results_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def get_non_rag_predictions(self):
        """è·å–ä¸ä½¿ç”¨ RAG çš„é¢„æµ‹ç»“æœ"""
        # åŠ è½½æ¨¡å‹
        base_model = "bigscience/bloom-560m"
        lora_path = Path(__file__).parent.parent.parent / "FinLLM-Instruction-tuning" / "results" / "finllm-lora"
        
        tokenizer = AutoTokenizer.from_pretrained(base_model)
        tokenizer.pad_token = tokenizer.eos_token
        
        from transformers import BitsAndBytesConfig
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        
        base = AutoModelForCausalLM.from_pretrained(
            base_model,
            quantization_config=quantization_config,
            device_map="auto"
        )
        
        model = PeftModel.from_pretrained(base, lora_path)
        model.eval()
        
        non_rag_results = []
        for item in self.results:
            query = item['query']
            # æ„å»ºä¸å¸¦ä¸Šä¸‹æ–‡çš„ prompt
            prompt = f"Human: Determine the sentiment of the financial news as negative, neutral or positive: {query}\nAssistant:"
            
            inputs = tokenizer(prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                output = model.generate(**inputs, max_new_tokens=20)
            
            prediction = tokenizer.decode(output[0], skip_special_tokens=True)
            prediction = prediction.split("Assistant:")[-1].strip()
            
            non_rag_results.append({
                "query": query,
                "prediction": prediction
            })
        
        # ä¿å­˜é RAG é¢„æµ‹ç»“æœ
        non_rag_path = self.output_dir / "non_rag_predictions.json"
        with open(non_rag_path, 'w', encoding='utf-8') as f:
            json.dump(non_rag_results, f, ensure_ascii=False, indent=2)
        logging.info(f"é RAG é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {non_rag_path}")
        
        return non_rag_results
    
    def analyze_retrieval_quality(self):
        """åˆ†ææ£€ç´¢è´¨é‡"""
        scores = [item['retrieval']['score'] for item in self.results]
        sources = [item['retrieval']['source'] for item in self.results]
        
        avg_score = sum(scores) / len(scores)
        logging.info(f"\nğŸ“Š æ£€ç´¢è´¨é‡åˆ†æ:")
        logging.info(f"å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {avg_score:.3f}")
        
        source_counts = {}
        for source in sources:
            source_counts[source] = source_counts.get(source, 0) + 1
        
        logging.info("\næ¥æºåˆ†å¸ƒ:")
        for source, count in source_counts.items():
            logging.info(f"{source}: {count} ç¯‡æ–‡ç« ")
        
        # ä¿å­˜æ£€ç´¢è´¨é‡åˆ†æç»“æœ
        retrieval_quality = {
            "average_score": avg_score,
            "source_distribution": source_counts
        }
        retrieval_path = self.output_dir / "retrieval_quality.json"
        with open(retrieval_path, 'w', encoding='utf-8') as f:
            json.dump(retrieval_quality, f, ensure_ascii=False, indent=2)
        logging.info(f"æ£€ç´¢è´¨é‡åˆ†æç»“æœå·²ä¿å­˜åˆ°: {retrieval_path}")
        
        return scores, sources
    
    def analyze_sentiment_distribution(self, results, label="RAG"):
        """åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ"""
        sentiments = [item['prediction'].lower() for item in results]
        sentiment_counts = {}
        for sentiment in sentiments:
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        logging.info(f"\nğŸ¯ {label} æƒ…æ„Ÿåˆ†å¸ƒ:")
        for sentiment, count in sentiment_counts.items():
            percentage = (count / len(sentiments)) * 100
            logging.info(f"{sentiment}: {count} æ¡ ({percentage:.1f}%)")
        
        # ä¿å­˜æƒ…æ„Ÿåˆ†å¸ƒåˆ†æç»“æœ
        sentiment_path = self.output_dir / f"{label.lower()}_sentiment_distribution.json"
        with open(sentiment_path, 'w', encoding='utf-8') as f:
            json.dump(sentiment_counts, f, ensure_ascii=False, indent=2)
        logging.info(f"{label} æƒ…æ„Ÿåˆ†å¸ƒåˆ†æç»“æœå·²ä¿å­˜åˆ°: {sentiment_path}")
        
        return sentiment_counts
    
    def plot_sentiment_comparison(self, rag_counts, non_rag_counts):
        """ç»˜åˆ¶ RAG å’Œé RAG çš„æƒ…æ„Ÿåˆ†å¸ƒå¯¹æ¯”å›¾"""
        plt.figure(figsize=(12, 6))
        
        # å‡†å¤‡æ•°æ®
        sentiments = sorted(set(rag_counts.keys()) | set(non_rag_counts.keys()))
        rag_values = [rag_counts.get(s, 0) for s in sentiments]
        non_rag_values = [non_rag_counts.get(s, 0) for s in sentiments]
        
        x = range(len(sentiments))
        width = 0.35
        
        plt.bar([i - width/2 for i in x], rag_values, width, label='With RAG')
        plt.bar([i + width/2 for i in x], non_rag_values, width, label='Without RAG')
        
        plt.xlabel('æƒ…æ„Ÿç±»åˆ«')
        plt.ylabel('æ•°é‡')
        plt.title('RAG vs Non-RAG æƒ…æ„Ÿåˆ†å¸ƒå¯¹æ¯”')
        plt.xticks(x, sentiments)
        plt.legend()
        
        plt.savefig(self.output_dir / 'sentiment_comparison.png')
        plt.close()
        logging.info(f"æƒ…æ„Ÿåˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {self.output_dir / 'sentiment_comparison.png'}")
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        # åˆ†ææ£€ç´¢è´¨é‡
        scores, sources = self.analyze_retrieval_quality()
        
        # è·å–é RAG é¢„æµ‹ç»“æœ
        non_rag_results = self.get_non_rag_predictions()
        
        # åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ
        rag_sentiment_counts = self.analyze_sentiment_distribution(self.results, "RAG")
        non_rag_sentiment_counts = self.analyze_sentiment_distribution(non_rag_results, "Non-RAG")
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾
        self.plot_sentiment_comparison(rag_sentiment_counts, non_rag_sentiment_counts)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            "total_queries": len(self.results),
            "retrieval_quality": {
                "average_score": sum(scores) / len(scores),
                "source_distribution": {source: sources.count(source) for source in set(sources)}
            },
            "sentiment_distribution": {
                "with_rag": rag_sentiment_counts,
                "without_rag": non_rag_sentiment_counts
            }
        }
        
        report_path = self.output_dir / "evaluation_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logging.info(f"\nğŸ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def main():
    # è·å–æ¨ç†ç»“æœæ–‡ä»¶è·¯å¾„
    inference_dir = Path(__file__).parent.parent / "inference"
    results_path = inference_dir / "rag_inference_results.json"
    
    if not results_path.exists():
        logging.error(f"æ‰¾ä¸åˆ°æ¨ç†ç»“æœæ–‡ä»¶: {results_path}")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨å¹¶ç”ŸæˆæŠ¥å‘Š
    evaluator = RAGEvaluator(results_path)
    evaluator.generate_report()

if __name__ == "__main__":
    main() 