import requests
from bs4 import BeautifulSoup
import urllib.parse
import json
from pathlib import Path
import time
import random
from fake_useragent import UserAgent
import logging

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ========== ç›¸ä¼¼åº¦å‡½æ•° ==========
def similarity_score(a, b):
    a_words, b_words = set(a.lower().split()), set(b.lower().split())
    return len(a_words & b_words) / (min(len(a_words), len(b_words)) + 1e-6)

# ========== ç”Ÿæˆéšæœº headers ==========
def get_random_headers():
    ua = UserAgent()
    return {
        "User-Agent": ua.random,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0"
    }

# ========== è¯·æ±‚ç½‘é¡µå†…å®¹ ==========
def make_request(url, max_retries=3):
    for i in range(max_retries):
        try:
            headers = get_random_headers()
            logging.info(f"å°è¯•è¯·æ±‚ URL: {url}")
            logging.info(f"ä½¿ç”¨ User-Agent: {headers['User-Agent']}")
            
            # å‡å°‘éšæœºå»¶è¿Ÿ
            delay = random.uniform(3, 5)
            logging.info(f"ç­‰å¾… {delay:.1f} ç§’...")
            time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            logging.info(f"è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            return response
        except requests.exceptions.RequestException as e:
            logging.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {i+1}/{max_retries}): {str(e)}")
            if i < max_retries - 1:
                time.sleep(random.uniform(5, 8))  # å¤±è´¥åç­‰å¾…æ—¶é—´ä¹Ÿå‡å°‘
            continue
    return None

# ========== æå–æ–‡ç« æ­£æ–‡ ==========
def scrape_article_text(url):
    try:
        response = make_request(url)
        if not response:
            return ""

        soup = BeautifulSoup(response.text, "html.parser")
        if "seekingalpha.com" in url:
            paras = soup.select("div.article-content p")
        elif "marketwatch.com" in url:
            paras = soup.select("p.article__body")
        else:
            paras = soup.find_all("p")

        if not paras:
            paras = soup.find_all("p")  # fallback

        text = " ".join(p.text.strip() for p in paras)
        return text[:1000] if text else ""
    except Exception as e:
        logging.error(f"æ–‡ç« å†…å®¹æŠ“å–é”™è¯¯: {str(e)}")
        return ""

# ========== Seeking Alpha æŠ“å– ==========
def scrape_seeking_alpha(query):
    try:
        # ä½¿ç”¨ Seeking Alpha æœç´¢
        url = f"https://seekingalpha.com/search?q={urllib.parse.quote(query)}"
        response = make_request(url)
        if not response:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select("div.article-list article")
        logging.info(f"æ‰¾åˆ° {len(articles)} ä¸ª Seeking Alpha æ–‡ç« ")

        for article in articles:
            try:
                title_tag = article.select_one("h3.title a")
                if not title_tag:
                    continue
                title = title_tag.text.strip()
                link = title_tag["href"]
                if not link.startswith("http"):
                    link = "https://seekingalpha.com" + link
                score = similarity_score(query, title)
                logging.info(f"æ ‡é¢˜: {title}, ç›¸ä¼¼åº¦: {score:.2f}")
                if score > 0.2:
                    context = scrape_article_text(link)
                    if context:
                        return {
                            "source": "Seeking Alpha",
                            "title": title,
                            "url": link,
                            "context": context
                        }
            except Exception as e:
                logging.warning(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                continue
    except Exception as e:
        logging.error(f"Seeking Alpha æŠ“å–é”™è¯¯: {str(e)}")
    return None

# ========== MarketWatch æŠ“å– ==========
def scrape_marketwatch(query):
    try:
        url = f"https://www.marketwatch.com/search?q={urllib.parse.quote(query)}&m=Keyword&rpp=15&mp=806&bd=false&rs=true"
        response = make_request(url)
        if not response:
            return None

        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.select("div.article__content")
        logging.info(f"æ‰¾åˆ° {len(articles)} ä¸ª MarketWatch æ–‡ç« ")

        for article in articles:
            try:
                title_tag = article.select_one("h3.article__headline a")
                if not title_tag:
                    continue
                title = title_tag.text.strip()
                link = title_tag["href"]
                if not link.startswith("http"):
                    link = "https://www.marketwatch.com" + link
                score = similarity_score(query, title)
                logging.info(f"æ ‡é¢˜: {title}, ç›¸ä¼¼åº¦: {score:.2f}")
                if score > 0.2:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä»˜è´¹å†…å®¹
                    if "barrons.com" in link:
                        logging.info("è·³è¿‡ä»˜è´¹å†…å®¹")
                        continue
                    context = scrape_article_text(link)
                    if context:
                        return {
                            "source": "MarketWatch",
                            "title": title,
                            "url": link,
                            "context": context
                        }
            except Exception as e:
                logging.warning(f"å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                continue
    except Exception as e:
        logging.error(f"MarketWatch æŠ“å–é”™è¯¯: {str(e)}")
    return None

# ========== ä¸»å‡½æ•° ==========
def main():
    queries = [
        "Tesla stock price movement",
        "JPMorgan stock analysis",
        "Apple earnings report",
        "Microsoft cloud growth",
        "Amazon quarterly results",
        "Goldman Sachs market outlook",
        "Bank of America financial results",
        "Meta AI development",
        "Federal Reserve interest rates",
        "Oil prices market update"
    ]

    results = []
    for query in queries:
        logging.info(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢: {query}")

        result = scrape_seeking_alpha(query) or scrape_marketwatch(query)

        if result:
            logging.info(f"âœ… [{result['source']}] æˆåŠŸè·å–æ–‡ç« : {result['title']}")
            results.append({
                "query": query,
                "retrieved": result
            })
        else:
            logging.warning("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡ç« ")

        # å‡å°‘æŸ¥è¯¢é—´éš”
        delay = random.uniform(2, 4)
        logging.info(f"â³ ç­‰å¾… {delay:.1f} ç§’...\n")
        time.sleep(delay)

    # ä¿å­˜ç»“æœåˆ°æ­£ç¡®çš„è·¯å¾„
    output_path = Path(__file__).parent / "rag_context_data.jsonl"
    with open(output_path, "w", encoding="utf-8") as f:
        for item in results:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

    logging.info(f"\nğŸ“ å·²ä¿å­˜ {len(results)} æ¡æ£€ç´¢ç»“æœåˆ° {output_path}")

if __name__ == "__main__":
    main()
